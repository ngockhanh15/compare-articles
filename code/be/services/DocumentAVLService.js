const { TreeAVL, TextHasher } = require("../utils/TreeAVL");
const Document = require("../models/Document");
const vietnameseStopwordService = require("./VietnameseStopwordService");

class DocumentAVLService {
  constructor() {
    this.documentTree = new TreeAVL();
    this.initialized = false;
  }

  // Initialize tree with existing documents
  async initialize() {
    if (this.initialized) return;

    try {
      console.log("Initializing Document AVL Tree...");

      // Initialize Vietnamese stopword service first
      if (!vietnameseStopwordService.initialized) {
        console.log("Initializing Vietnamese Stopword Service...");
        await vietnameseStopwordService.initialize();
      }

      // Load all processed documents from database
      const documents = await Document.find({
        status: "processed",
        extractedText: { $exists: true, $ne: "" },
      }).select("_id title fileType extractedText createdAt uploadedBy");

      console.log(`Loading ${documents.length} documents into AVL tree...`);

      if (documents.length === 0) {
        console.log("‚ö†Ô∏è  No processed documents found in database!");
        this.initialized = true;
        return;
      }

      for (const doc of documents) {
        // Only add to tree, don't regenerate AVL tree data if it already exists
        await this.addDocumentToTreeOnly(doc);
      }

      this.initialized = true;
      console.log(
        `Document AVL Tree initialized with ${this.documentTree.getSize()} entries`
      );
    } catch (error) {
      console.error("Error initializing Document AVL Tree:", error);
      throw error;
    }
  }

  // Add document to AVL tree
  async addDocumentToTree(document) {
    try {
      if (
        !document.extractedText ||
        document.extractedText.trim().length === 0
      ) {
        return;
      }

      // Create composite key for sorting: fileType + createdAt + _id
      const sortKey = this.createSortKey(document);

      // Create word hashes from document text (using new method)
      const wordHashes = TextHasher.createWordHashes(
        document.extractedText,
        true
      );

      // Store document metadata with word hashes
      const documentData = {
        documentId: document._id,
        title: document.title,
        fileType: document.fileType,
        createdAt: document.createdAt,
        uploadedBy: document.uploadedBy,
        textLength: document.extractedText.length,
        wordCount: document.extractedText.split(/\s+/).length,
        wordHashes: wordHashes,
        fullText: document.extractedText,
        sortKey: sortKey,
      };

      // Insert each word hash into tree (not the sortKey!)
      for (const wordHash of wordHashes) {
        this.documentTree.insert(wordHash.hash, documentData);
      }

      console.log(
        `Added document "${document.title}" to AVL tree with ${wordHashes.length} word hashes`
      );

      // Return AVL tree data for saving to database
      return this.generateAVLTreeData(document, sortKey, wordHashes);
    } catch (error) {
      console.error(`Error adding document ${document._id} to tree:`, error);
      return null;
    }
  }

  // Add document to AVL tree only (for initialization)
  async addDocumentToTreeOnly(document) {
    try {
      if (
        !document.extractedText ||
        document.extractedText.trim().length === 0
      ) {
        return;
      }

      // Create composite key for sorting: fileType + createdAt + _id
      const sortKey = this.createSortKey(document);

      // Create word hashes from document text (using new method)
      const wordHashes = TextHasher.createWordHashes(
        document.extractedText,
        true
      );

      // Store document metadata with word hashes
      const documentData = {
        documentId: document._id,
        title: document.title,
        fileType: document.fileType,
        createdAt: document.createdAt,
        uploadedBy: document.uploadedBy,
        textLength: document.extractedText.length,
        wordCount: document.extractedText.split(/\s+/).length,
        wordHashes: wordHashes,
        fullText: document.extractedText,
        sortKey: sortKey,
      };

      // Insert each word hash into tree (not the sortKey!)
      for (const wordHash of wordHashes) {
        this.documentTree.insert(wordHash.hash, documentData);
      }

      console.log(
        `Added document "${document.title}" to AVL tree with ${wordHashes.length} word hashes`
      );
    } catch (error) {
      console.error(`Error adding document ${document._id} to tree:`, error);
    }
  }

  // Generate AVL tree data as hash vector for database storage
  generateAVLTreeData(document, sortKey, wordHashes) {
    try {
      // Create hash vector representation of the document's position in AVL tree
      const avlTreeData = {
        sortKey: sortKey,
        hashVector: wordHashes.map((wordHash) => ({
          hash: wordHash.hash,
          word: wordHash.word,
          index: wordHash.index,
          method: wordHash.method,
        })),
        treeMetadata: {
          documentId: document._id,
          insertedAt: new Date(),
          textLength: document.extractedText.length,
          wordCount: wordHashes.length,
          fileTypeWeight: this.getFileTypeWeight(document.fileType),
        },
      };

      return avlTreeData;
    } catch (error) {
      console.error("Error generating AVL tree data:", error);
      return null;
    }
  }

  // Create composite sort key: fileType + timestamp + id
  createSortKey(document) {
    const fileTypeWeight = this.getFileTypeWeight(document.fileType);
    const timestamp = new Date(document.createdAt).getTime();
    const idHash = document._id.toString().slice(-8); // Last 8 chars of ID

    // Format: fileTypeWeight-timestamp-idHash
    return `${fileTypeWeight
      .toString()
      .padStart(2, "0")}-${timestamp}-${idHash}`;
  }

  // Assign weights to file types for sorting
  getFileTypeWeight(fileType) {
    const weights = {
      pdf: 1,
      docx: 2,
      doc: 3,
      txt: 4,
      xlsx: 5,
      xls: 6,
      pptx: 7,
      ppt: 8,
    };
    return weights[fileType] || 9;
  }

  // Check for duplicate content using AVL tree with hash-based plagiarism detection
  // Trong ph∆∞∆°ng th·ª©c checkDuplicateContent c·ªßa DocumentAVLService
async checkDuplicateContent(text, options = {}) {
  if (!this.initialized) {
    await this.initialize();
  }

  // Ensure Vietnamese stopword service is initialized
  if (!vietnameseStopwordService.initialized) {
    await vietnameseStopwordService.initialize();
  }

  const { minSimilarity = 50, chunkSize = 2, maxResults = 10 } = options;

  try {
    console.log(`üîç Starting plagiarism check using AVL tree...`);
    console.log(`üìù Input text length: ${text.length} characters`);

    // B∆∞·ªõc 1: M√£ h√≥a vƒÉn b·∫£n input th√†nh word hashes
    const inputHashes = TextHasher.createWordHashes(text, true);

    // T·∫°o danh s√°ch c√°c t·ª´ c√≥ nghƒ©a (kh√¥ng l·∫∑p l·∫°i) t·ª´ vƒÉn b·∫£n ƒë·∫ßu v√†o
    const meaningfulWords = vietnameseStopwordService.extractMeaningfulWords(text);
    const uniqueInputWords = new Set(meaningfulWords);
    
    // T·∫°o danh s√°ch c√°c c·∫∑p t·ª´ t·ª´ vƒÉn b·∫£n ƒë·∫ßu v√†o
    const wordPairs = [];
    for (let i = 0; i < meaningfulWords.length - 1; i++) {
      wordPairs.push(`${meaningfulWords[i]}_${meaningfulWords[i+1]}`);
    }
    const uniqueWordPairs = new Set(wordPairs);
    
    console.log(`üìä Input text has ${uniqueInputWords.size} unique words and ${uniqueWordPairs.size} unique word pairs`);

    // B∆∞·ªõc 2: T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u
    const inputSentences = TextHasher.extractSentences(text);
    console.log(`üìù Input text has ${inputSentences.length} sentences`);

    // B∆∞·ªõc 3: T√¨m ki·∫øm c√°c hash trong c√¢y AVL
    const documentMatches = new Map(); // documentId -> {matches, totalHashes}
    let totalSearches = 0;

    for (const wordHash of inputHashes) {
      totalSearches++;
      const foundNode = this.documentTree.search(wordHash.hash);

      if (foundNode) {
        const docData = foundNode.data;
        const documentId = docData.documentId.toString(); // Ensure string for consistent key

        if (!documentMatches.has(documentId)) {
          documentMatches.set(documentId, {
            documentData: docData,
            matchedHashes: 0,
            totalInputHashes: inputHashes.length,
            matchedWords: [],
            matchedWordSet: new Set(), // Track unique words to avoid duplicates
            matchedSentences: [], // L∆∞u c√°c c√¢u tr√πng l·∫∑p
          });
        }

        const matchData = documentMatches.get(documentId);

        // Only count unique words to avoid inflating the match count
        if (!matchData.matchedWordSet.has(wordHash.word)) {
          matchData.matchedHashes++;
          matchData.matchedWords.push(wordHash.word);
          matchData.matchedWordSet.add(wordHash.word);
        }
      }
    }

    console.log(`üîç Searched ${totalSearches} hashes, found matches in ${documentMatches.size} documents`);
    console.log(`üå≥ AVL Tree size: ${this.documentTree.getSize()} documents`);

    // B∆∞·ªõc 4: Ph√¢n t√≠ch t·ª´ng c√¢u trong m·ªói t√†i li·ªáu ƒë·ªÉ t√¨m c√¢u tr√πng l·∫∑p
    let totalDuplicateSentences = 0;
    let totalSentencesWithInputWords = 0;
    
    for (const [documentId, matchData] of documentMatches.entries()) {
      const docData = matchData.documentData;
      
      // T√°ch t√†i li·ªáu th√†nh c√°c c√¢u
      const docSentences = TextHasher.extractSentences(docData.fullText);
      
      // Ph√¢n t√≠ch t·ª´ng c√¢u
      for (const sentence of docSentences) {
        // T√°ch c√¢u th√†nh c√°c t·ª´ c√≥ nghƒ©a
        const sentenceWords = vietnameseStopwordService.extractMeaningfulWords(sentence);
        const uniqueSentenceWords = new Set(sentenceWords);
        
        // T·∫°o c√°c c·∫∑p t·ª´ trong c√¢u
        const sentenceWordPairs = [];
        for (let i = 0; i < sentenceWords.length - 1; i++) {
          sentenceWordPairs.push(`${sentenceWords[i]}_${sentenceWords[i+1]}`);
        }
        const uniqueSentenceWordPairs = new Set(sentenceWordPairs);
        
        // T√¨m c√°c c·∫∑p t·ª´ tr√πng l·∫∑p (ƒë·ªìng nh·∫•t v·ªõi plagiarismController.js)
        const matchedWordPairs = [...uniqueSentenceWordPairs].filter((pair) =>
          uniqueWordPairs.has(pair)
        );
        
        // T√≠nh t·ª∑ l·ªá tr√πng l·∫∑p theo c√¥ng th·ª©c: (s·ªë c·∫∑p t·ª´ tr√πng / s·ªë c·∫∑p t·ª´ trong c√¢u) * 100 (ƒë·ªìng nh·∫•t v·ªõi plagiarismController.js)
        let duplicateRatio = 0;
        if (uniqueSentenceWordPairs.size > 0) {
          duplicateRatio = (matchedWordPairs.length / uniqueSentenceWordPairs.size) * 100;
        }
        
        // Ki·ªÉm tra xem c√¢u c√≥ tr√πng l·∫∑p kh√¥ng theo ti√™u ch√≠ m·ªõi
        const isDuplicate = duplicateRatio > 50;
        
        // ƒê·∫øm s·ªë t·ª´ tr√πng l·∫∑p (ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi logic c≈©)
        const matchedWords = [...uniqueSentenceWords].filter(word => uniqueInputWords.has(word));
        
        // N·∫øu c√¢u tr√πng l·∫∑p, th√™m v√†o danh s√°ch
        if (isDuplicate) {
          matchData.matchedSentences.push({
            sentence,
            duplicateRatio,
            matchedWordPairs, // S·ª≠ d·ª•ng matchedWordPairs thay v√¨ matchedWords
            matchedWords, // Gi·ªØ l·∫°i ƒë·ªÉ t∆∞∆°ng th√≠ch
            totalWordPairs: uniqueSentenceWordPairs.size,
            matchedWordPairsCount: matchedWordPairs.length
          });
          totalDuplicateSentences++;
        }
        
        // N·∫øu c√¢u c√≥ ch·ª©a t·ª´ t·ª´ vƒÉn b·∫£n ƒë·∫ßu v√†o
        if (matchedWords.length > 0) {
          totalSentencesWithInputWords++;
        }
      }
    }

    // B∆∞·ªõc 5: T√≠nh plagiarism ratio cho t·ª´ng document
    const matches = [];
    const allMatches = []; // L∆∞u t·∫•t c·∫£ matches ƒë·ªÉ t√≠nh Dtotal
    const processedDocuments = new Set(); // Track processed document IDs

    for (const [documentId, matchData] of documentMatches.entries()) {
      const { documentData, matchedHashes, totalInputHashes, matchedWords, matchedSentences } = matchData;

      // Skip if document already processed (additional safety check)
      if (processedDocuments.has(documentId)) {
        console.log(`‚ö†Ô∏è Skipping duplicate document: ${documentData.title} (ID: ${documentId})`);
        continue;
      }

      // T√≠nh sentence-level similarity = trung b√¨nh similarity c·ªßa c√°c c√¢u tr√πng l·∫∑p
      let plagiarismRatio = 0;
      if (matchedSentences.length > 0) {
        const totalSimilarity = matchedSentences.reduce((sum, sentence) => sum + sentence.duplicateRatio, 0);
        plagiarismRatio = Math.round(totalSimilarity / matchedSentences.length);
      }

      console.log(`üìä Document "${documentData.title}": ${matchedSentences.length} duplicate sentences with average similarity = ${plagiarismRatio}%`);
      console.log(`üî§ Matched words: ${matchedWords.slice(0, 10).join(", ")}${matchedWords.length > 10 ? "..." : ""}`);
      console.log(`üìù Duplicate sentences: ${matchedSentences.length}`);

      // T·∫°o match object cho t·∫•t c·∫£ documents c√≥ matches
      const matchObject = {
        documentId: documentData.documentId,
        title: documentData.title,
        fileType: documentData.fileType,
        createdAt: documentData.createdAt,
        similarity: plagiarismRatio,
        matchedHashes: matchedHashes,
        totalHashes: totalInputHashes,
        matchedWords: matchedWords,
        matchedText: documentData.fullText.substring(0, 500) + "...", // Preview
        inputText: text.substring(0, 500) + "...", // Preview
        method: "sentence-level-similarity",
        source: "document-avl-tree",
        duplicateSentences: matchedSentences.length, // S·ªë c√¢u tr√πng l·∫∑p
        duplicateSentencesDetails: matchedSentences.slice(0, 5) // Chi ti·∫øt 5 c√¢u tr√πng ƒë·∫ßu ti√™n
      };

      // Ch·ªâ x·ª≠ l√Ω documents c√≥ c√¢u tr√πng l·∫∑p (sentence-level similarity > 0)
      if (matchedSentences.length > 0) {
        // Th√™m v√†o allMatches ƒë·ªÉ t√≠nh Dtotal
        allMatches.push(matchObject);

        // Ch·ªâ th√™m v√†o matches ch√≠nh n·∫øu v∆∞·ª£t threshold
        if (plagiarismRatio >= minSimilarity) {
          console.log(`‚úÖ Document "${documentData.title}" exceeds threshold (${plagiarismRatio}% >= ${minSimilarity}%)`);
          matches.push(matchObject);
        }
      } else {
        console.log(`‚ö†Ô∏è Document "${documentData.title}" has no duplicate sentences, skipping...`);
      }

      processedDocuments.add(documentId);
    }

    // Sort by plagiarism ratio (similarity)
    matches.sort((a, b) => b.similarity - a.similarity);

    // T√¨m t√†i li·ªáu c√≥ s·ªë l∆∞·ª£ng c√¢u tr√πng l·∫∑p nhi·ªÅu nh·∫•t
    let maxDuplicateSentences = 0;
    let documentWithMostDuplicates = null;

    for (const match of allMatches) {
      if (match.duplicateSentences > maxDuplicateSentences) {
        maxDuplicateSentences = match.duplicateSentences;
        documentWithMostDuplicates = {
          id: match.documentId,
          name: match.title,
          title: match.title,
          fileName: match.title, // Fallback
          duplicateSentences: match.duplicateSentences
        };
      }
    }

    // Calculate overall duplicate percentage
    const duplicatePercentage = this.calculatePlagiarismRatio(inputHashes.length, matches);

    // T√≠nh to√°n Dtotal v√† DA/B d·ª±a tr√™n hash matches
    const { dtotal, dab, mostSimilarDocument } = this.calculateDtotalAndDABFromHashes(inputHashes, allMatches);

    const result = {
      duplicatePercentage,
      matches: matches,
      totalMatches: matches.length,
      checkedDocuments: this.documentTree.getSize(),
      totalDocumentsInSystem: this.documentTree.getSize(),
      sources: [...new Set(matches.map((m) => m.title))],
      confidence: duplicatePercentage > 70 ? "high" : duplicatePercentage > 30 ? "medium" : "low",
      // Th√™m c√°c th√¥ng s·ªë m·ªõi
      dtotal,
      dab,
      mostSimilarDocument,
      // Th√¥ng tin v·ªÅ qu√° tr√¨nh m√£ h√≥a
      totalInputHashes: inputHashes.length,
      searchMethod: "avl-tree-word-hash-based",
      // Th√™m 2 th√¥ng s·ªë theo y√™u c·∫ßu
      totalSentencesWithInputWords: totalSentencesWithInputWords,
      maxDuplicateSentences: maxDuplicateSentences,
      documentWithMostDuplicates: documentWithMostDuplicates,
      // Th√¥ng tin v·ªÅ c·∫∑p t·ª´
      totalUniqueWordPairs: uniqueWordPairs.size,
      totalUniqueWords: uniqueInputWords.size,
      totalDuplicateSentences: totalDuplicateSentences
    };

    console.log(`üìä Final result summary:`, {
      duplicatePercentage: result.duplicatePercentage,
      totalMatches: result.totalMatches,
      checkedDocuments: result.checkedDocuments,
      totalSentencesWithInputWords: result.totalSentencesWithInputWords,
      maxDuplicateSentences: result.maxDuplicateSentences,
      totalDuplicateSentences: result.totalDuplicateSentences
    });

    return result;
  } catch (error) {
    console.error("Error checking duplicate content:", error);
    throw error;
  }
}


  // Calculate plagiarism ratio based on hash matches
  calculatePlagiarismRatio(totalInputHashes, matches) {
    if (matches.length === 0 || totalInputHashes === 0) return 0;

    // Tr·∫£ v·ªÅ t·ª∑ l·ªá c·ªßa document c√≥ similarity cao nh·∫•t (ƒë·∫£m b·∫£o nh·∫•t qu√°n)
    const highestMatch = matches[0]; // matches ƒë√£ ƒë∆∞·ª£c sort theo similarity desc
    const result = highestMatch ? highestMatch.similarity : 0;

    console.log(
      `üéØ Overall duplicate percentage: ${result}% (based on highest match: ${highestMatch?.title})`
    );
    return result;
  }

  // Calculate Dtotal and DA/B from hash matches
  // Calculate Dtotal and DA/B from hash matches
  calculateDtotalAndDABFromHashes(inputHashes, matches) {
    console.log(
      `üîç calculateDtotalAndDABFromHashes called with ${matches.length} matches`
    );

    if (matches.length === 0) {
      console.log(`‚ö†Ô∏è No matches found, returning dtotal=0`);
      return {
        dtotal: 0,
        dab: 0,
        mostSimilarDocument: null,
      };
    }

    // L·∫•y vƒÉn b·∫£n ƒë·∫ßu v√†o t·ª´ match ƒë·∫ßu ti√™n (t·∫•t c·∫£ matches ƒë·ªÅu c√≥ c√πng inputText)
    const inputText = matches[0].inputText.replace(/\.\.\.$/, ""); // Lo·∫°i b·ªè d·∫•u "..." ·ªü cu·ªëi

    // Chia vƒÉn b·∫£n th√†nh c√°c c√¢u
    const sentences = inputText
      .split(/[.!?]+/)
      .filter((sentence) => sentence.trim().length > 0);

    console.log(`üìù Input text has ${sentences.length} sentences`);

    // T·∫≠p h·ª£p c√°c c√¢u tr√πng l·∫∑p (s·ª≠ d·ª•ng Set ƒë·ªÉ tr√°nh tr√πng l·∫∑p)
    const duplicateSentences = new Set();

    // T·∫°o map t·ª´ hash ƒë·∫øn t·ª´ ƒë·ªÉ d·ªÖ d√†ng ki·ªÉm tra
    const hashToWordMap = new Map();
    inputHashes.forEach((hash) => {
      hashToWordMap.set(hash.hash, hash.word);
    });

    // Ki·ªÉm tra m·ªói c√¢u v·ªõi c√°c t·ª´ ƒë√£ t√¨m th·∫•y trong c√¢y AVL
    sentences.forEach((sentence, sentenceIndex) => {
      // T·∫°o danh s√°ch c√°c t·ª´ trong c√¢u
      const sentenceWords = TextHasher.createWordHashes(sentence, true);

      // ƒê·∫øm s·ªë t·ª´ trong c√¢u n√†y ƒë√£ ƒë∆∞·ª£c t√¨m th·∫•y trong c√¢y AVL
      let matchedWordsInSentence = 0;
      let totalWordsInSentence = sentenceWords.length;

      // Ki·ªÉm tra t·ª´ng t·ª´ trong c√¢u
      sentenceWords.forEach((wordHash) => {
        // N·∫øu t·ª´ n√†y ƒë√£ ƒë∆∞·ª£c t√¨m th·∫•y trong b·∫•t k·ª≥ document n√†o
        if (
          matches.some(
            (match) =>
              match.matchedWords && match.matchedWords.includes(wordHash.word)
          )
        ) {
          matchedWordsInSentence++;
        }
      });

      // N·∫øu t·ª∑ l·ªá t·ª´ tr√πng l·∫∑p trong c√¢u v∆∞·ª£t ng∆∞·ª°ng (v√≠ d·ª•: 50%)
      if (
        totalWordsInSentence > 0 &&
        matchedWordsInSentence / totalWordsInSentence >= 0.5
      ) {
        duplicateSentences.add(sentenceIndex);
      }
    });

    console.log(
      `üî§ Total duplicate sentences: ${duplicateSentences.size} out of ${sentences.length}`
    );

    // Sort matches by similarity ƒë·ªÉ t√¨m document gi·ªëng nh·∫•t
    const sortedMatches = [...matches].sort(
      (a, b) => b.similarity - a.similarity
    );

    // DA/B: s·ªë hash tr√πng v·ªõi document gi·ªëng nh·∫•t (document c√≥ similarity cao nh·∫•t)
    const mostSimilarMatch = sortedMatches[0];
    const dab = mostSimilarMatch ? mostSimilarMatch.matchedHashes : 0;

    const mostSimilarDocument = mostSimilarMatch
      ? {
          id: mostSimilarMatch.documentId,
          name: mostSimilarMatch.title,
          similarity: mostSimilarMatch.similarity,
        }
      : null;

    console.log(
      `üéØ Most similar document: "${mostSimilarMatch?.title}" with ${dab} matched hashes`
    );

    return {
      dtotal: duplicateSentences.size, // S·ªë c√¢u tr√πng v·ªõi to√†n b·ªô documents
      dab: dab, // S·ªë t·ª´ tr√πng v·ªõi document gi·ªëng nh·∫•t
      mostSimilarDocument: mostSimilarDocument,
    };
  }

  // Calculate text similarity using Plagiarism Ratio formula
  calculateTextSimilarity(text1, text2) {
    const words1 = text1
      .toLowerCase()
      .split(/\s+/)
      .filter((w) => w.length > 2);
    const words2 = text2
      .toLowerCase()
      .split(/\s+/)
    // S·ª≠ d·ª•ng c√¥ng th·ª©c Plagiarism Ratio: (intersection.length / set1.size) * 100%  .filter((w) => w.length > 2);

    if (words1.length === 0 || words2.length === 0) return 0;

    const set1 = new Set(words1);

    t(words2);

    const intersection = new Set([...set1].filter((x) => set2.has(x)));

    // S·ª≠ d·ª•ng c√¥ng th·ª©c Plagiarism Ratio: (intersection.length / set1.size) * 100%
    return Math.round((intersection.size / set1.size) * 100);
  }

  // Calculate overall duplicate percentage
  calculateOverallDuplicatePercentage(originalText, matches) {
    if (matches.length === 0) return 0;

    // N·∫øu c√≥ match v·ªõi similarity 100%, tr·∫£ v·ªÅ 100%
    const perfectMatch = matches.find((match) => match.similarity >= 100);
    if (perfectMatch) return 100;

    // N·∫øu kh√¥ng c√≥ match ho√†n h·∫£o, l·∫•y similarity cao nh·∫•t
    const highestSimilarity = Math.max(
      ...matches.map((match) => match.similarity)
    );
    return highestSimilarity;
  }

  // T√≠nh to√°n Dtotal v√† DA/B
  calculateDtotalAndDAB(originalText, matches) {
    if (matches.length === 0) {
      return {
        dtotal: 0,
        dab: 0,
        mostSimilarDocument: null,
      };
    }

    // T√°ch c√¢u t·ª´ vƒÉn b·∫£n g·ªëc
    const originalSentences = originalText
      .split(/[.!?]+/)
      .map((s) => s.trim())
      .filter((s) => s.length > 10);

    // T√≠nh Dtotal: s·ªë c√¢u duy nh·∫•t tr√πng v·ªõi to√†n b·ªô CSDL
    const uniqueMatchedSentences = new Set();

    // T√¨m document c√≥ similarity cao nh·∫•t
    let mostSimilarMatch = null;
    let highestSimilarity = 0;

    matches.forEach((match) => {
      // Th√™m c√¢u v√†o set ƒë·ªÉ t√≠nh Dtotal (tr√°nh tr√πng l·∫∑p)
      const matchSentences = match.matchedText
        .split(/[.!?]+/)
        .map((s) => s.trim())
        .filter((s) => s.length > 10);

      // So s√°nh t·ª´ng c√¢u trong vƒÉn b·∫£n g·ªëc v·ªõi c√¢u trong match
      originalSentences.forEach((origSentence) => {
        matchSentences.forEach((matchSentence) => {
          const similarity = this.calculateTextSimilarity(
            origSentence,
            matchSentence
          );
          if (similarity >= 50) {
            // Threshold 50% ƒë·ªÉ coi l√† tr√πng
            uniqueMatchedSentences.add(origSentence.toLowerCase());
          }
        });
      });

      // T√¨m document c√≥ similarity cao nh  ·∫•t cho DA/B
      if (match.similarity > highestSimilarity) {
        highestSimilarity = match.similarity;
        mostSimilarMatch = match;
      }
    });

    // T√≠nh DA/B: s·ªë c√¢u tr√πng v·ªõi document gi·ªëng nh·∫•t
    let dab = 0;
    let mostSimilarDocument = null;

    if (mostSimilarMatch) {
      const mostSimilarSentences = mostSimilarMatch.matchedText
        .split(/[.!?]+/)
        .map((s) => s.trim())
        .filter((s) => s.length > 10);

      // ƒê·∫øm s·ªë c√¢u trong vƒÉn b·∫£n g·ªëc tr√πng v·ªõi document gi·ªëng nh·∫•t
      const matchedWithMostSimilar = new Set();

      originalSentences.forEach((origSentence) => {
        mostSimilarSentences.forEach((simSentence) => {
          const similarity = this.calculateTextSimilarity(
            origSentence,
            simSentence
          );
          if (similarity >= 50) {
            // Threshold 50%
            matchedWithMostSimilar.add(origSentence.toLowerCase());
          }
        });
      });

      dab = matchedWithMostSimilar.size;

      mostSimilarDocument = {
        id: mostSimilarMatch.documentId,
        name: mostSimilarMatch.title,
        similarity: mostSimilarMatch.similarity,
      };
    }

    return {
      dtotal: uniqueMatchedSentences.size,
      dab: dab,
      mostSimilarDocument: mostSimilarDocument,
    };
  }

  // Get tree statistics
  getTreeStats() {
    if (!this.initialized) {
      return {
        totalDocuments: 0,
        totalSentences: 0,
        totalNodes: 0,
        treeSize: 0,
        initialized: false,
        uniqueSentences: 0,
        uniquePercentage: 0,
        duplicateSentences: 0,
        duplicatePercentage: 0,
      };
    }

    const allNodes = this.documentTree.getAllNodes();
    const uniqueDocuments = new Set();
    const fileTypeStats = {};
    let totalSentences = 0;

    // ƒê·∫øm s·ªë c√¢u duy nh·∫•t (kh√¥ng tr√πng l·∫∑p)
    const hashCounts = new Map();

    allNodes.forEach((node) => {
      const documentId = node.data.documentId;
      const fileType = node.data.fileType;
      const hash = node.key; // Gi·∫£ s·ª≠ key l√† hash c·ªßa c√¢u

      // Count unique documents
      uniqueDocuments.add(documentId.toString());

      // Count file types (based on nodes, not documents)
      fileTypeStats[fileType] = (fileTypeStats[fileType] || 0) + 1;

      // Count sentences/word hashes
      totalSentences++;

      // ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói hash
      hashCounts.set(hash, (hashCounts.get(hash) || 0) + 1);
    });

    // ƒê·∫øm s·ªë c√¢u duy nh·∫•t (hash ch·ªâ xu·∫•t hi·ªán m·ªôt l·∫ßn)
    let uniqueSentences = 0;
    hashCounts.forEach((count) => {
      if (count === 1) {
        uniqueSentences++;
      }
    });

    // T√≠nh ph·∫ßn trƒÉm c√¢u duy nh·∫•t
    const uniquePercentage =
      totalSentences > 0 ? (uniqueSentences / totalSentences) * 100 : 0;
    const duplicateSentences = totalSentences - uniqueSentences;
    const duplicatePercentage = 100 - uniquePercentage;

    return {
      totalDocuments: uniqueDocuments.size, // S·ªë l∆∞·ª£ng documents duy nh·∫•t
      totalSentences: totalSentences, // T·ªïng s·ªë word hashes/sentences
      totalNodes: this.documentTree.getSize(), // T·ªïng s·ªë nodes trong tree
      treeSize: this.documentTree.getSize(),
      initialized: this.initialized,
      fileTypeDistribution: fileTypeStats,
      treeHeight: this.getTreeHeight(),
      uniqueSentences: uniqueSentences, // S·ªë c√¢u duy nh·∫•t (kh√¥ng tr√πng l·∫∑p)
      uniquePercentage: uniquePercentage.toFixed(2), // Ph·∫ßn trƒÉm c√¢u duy nh·∫•t
      duplicateSentences: duplicateSentences, // S·ªë c√¢u tr√πng l·∫∑p
      duplicatePercentage: duplicatePercentage.toFixed(2), // Ph·∫ßn trƒÉm c√¢u tr√πng l·∫∑p
    };
  }

  // Get tree height
  getTreeHeight() {
    if (!this.documentTree.root) return 0;
    return this.documentTree.getHeight(this.documentTree.root);
  }

  // Remove document from tree (when document is deleted)
  async removeDocumentFromTree(documentId) {
    try {
      // Since AVL tree doesn't have direct delete by value,
      // we need to rebuild the tree without this document
      const allNodes = this.documentTree.getAllNodes();
      const filteredNodes = allNodes.filter(
        (node) => node.data.documentId.toString() !== documentId.toString()
      );

      // Clear and rebuild tree
      this.documentTree.clear();

      for (const node of filteredNodes) {
        this.documentTree.insert(node.hash, node.data);
      }

      console.log(`Removed document ${documentId} from AVL tree`);
    } catch (error) {
      console.error(`Error removing document ${documentId} from tree:`, error);
    }
  }

  // Refresh tree (reload from database)
  async refreshTree() {
    this.documentTree.clear();
    this.initialized = false;
    await this.initialize();
  }
}

// Create singleton instance
const documentAVLService = new DocumentAVLService();

module.exports = documentAVLService;
