const { TreeAVL, TextHasher } = require("../utils/TreeAVL");
const Document = require("../models/Document");
const GlobalAVLTreeUnified = require("../models/GlobalAVLTreeUnified");
const TokenizedWord = require("../models/TokenizedWord");
const Threshold = require("../models/Threshold");
const vietnameseStopwordService = require("./VietnameseStopwordService");

class DocumentAVLService {
  constructor() {
    this.documentTree = new TreeAVL();
    this.initialized = false;
    this.initializing = false; // Flag to prevent multiple simultaneous initializations
    this.docInfo = new Map(); // docId -> metadata incl. sentenceCount
    this.tokenizationSamples = []; // L∆∞u tr·ªØ tokenization samples
    this.autoSave = false; // T·∫Øt auto-save ƒë·ªÉ tr√°nh spam console
    this.saveInterval = 5 * 60 * 1000; // Save m·ªói 5 ph√∫t
    this.lastSaved = null;
    this.autoInitialize = false; // T·∫Øt t·ª± ƒë·ªông kh·ªüi t·∫°o khi startup - DISABLED

    // Inject this service into cache service ƒë·ªÉ th·ªëng nh·∫•t c√¢y AVL
    this.setupCacheService();

    // Auto-save timer - DISABLED
    // this.setupAutoSave();
  }

  setupCacheService() {
    try {
      const plagiarismCacheService = require('./PlagiarismCacheService');
      plagiarismCacheService.setDocumentAVLService(this);
    } catch (error) {
      console.warn('Could not setup cache service integration:', error.message);
    }
  }

  setupAutoSave() {
    if (this.autoSave) {
      setInterval(async () => {
        if (this.initialized && this.hasChanges()) {
          try {
            console.log('Auto-saving Global AVL Tree...');
            await this.saveToDatabase();
          } catch (error) {
            console.error('Auto-save failed:', error.message);
          }
        }
      }, this.saveInterval);
    }
  }

  // Check if there are changes since last save
  hasChanges() {
    if (!this.lastSaved) return true; // Never saved before
    
    // Check if any documents were added since last save
    const hasNewDocuments = Array.from(this.docInfo.values()).some(doc => 
      doc.createdAt > this.lastSaved
    );
    
    return hasNewDocuments;
  }

  // Initialize tree with existing documents
  async initialize() {
    if (this.initialized) {
      console.log("Document AVL Tree already initialized, skipping...");
      return;
    }

    // Prevent multiple simultaneous initializations
    if (this.initializing) {
      console.log("Document AVL Tree initialization in progress, waiting...");
      while (this.initializing) {
        await new Promise(resolve => setTimeout(resolve, 100));
      }
      return;
    }

    this.initializing = true;

    try {
      console.log("Initializing Document AVL Tree...");

      // Initialize Vietnamese stopword service first
      if (!vietnameseStopwordService.initialized) {
        console.log("Initializing Vietnamese Stopword Service...");
        await vietnameseStopwordService.initialize();
      }

      // Ki·ªÉm tra flag autoInitialize tr∆∞·ªõc khi t·ª± ƒë·ªông rebuild documents
      if (!this.autoInitialize) {
        console.log("üö´ Auto-initialization is DISABLED. Loading existing tree from database only...");
        
        // V·∫´n c·∫ßn load t·ª´ database ƒë·ªÉ c√≥ d·ªØ li·ªáu cho plagiarism detection
        const loadedFromDB = await this.loadFromDatabase();
        
        if (loadedFromDB) {
          console.log("‚úÖ Loaded existing Global AVL Tree from database");
        } else {
          console.log("‚ö†Ô∏è  No existing tree data found in database. Tree will be empty until manually loaded.");
        }
        
        this.initialized = true;
        this.initializing = false;
        return;
      }

      // Try to load from database first
      const loadedFromDB = await this.loadFromDatabase();

      if (loadedFromDB) {
        console.log("‚úÖ Loaded Global AVL Tree from database");
        this.initialized = true;
        this.initializing = false;
        return;
      }

      // If no database backup, rebuild from documents
      console.log("No database backup found, rebuilding from documents...");
      await this.rebuildFromDocuments();

      // Save initial tree to database
      await this.saveToDatabase();

      this.initialized = true;
      console.log(
        `Document AVL Tree initialized with ${this.documentTree.getSize()} entries`
      );
    } catch (error) {
      console.error("Error initializing Document AVL Tree:", error);
      this.initialized = false;
      throw error;
    } finally {
      this.initializing = false;
    }
  }

  // Rebuild tree from documents in database
  async rebuildFromDocuments() {
    // Load all processed documents from database
    const documents = await Document.find({
      status: "processed",
      extractedText: { $exists: true, $ne: "" },
    }).select("_id title fileType extractedText createdAt uploadedBy");

    console.log(`üîÑ Loading ${documents.length} documents into AVL tree...`);

    if (documents.length === 0) {
      console.log("‚ö†Ô∏è  No processed documents found in database!");
      return;
    }

    // Process documents without spamming console
    for (const doc of documents) {
      await this.addDocumentToTreeOnly(doc);
    }

    console.log(`‚úÖ Successfully loaded ${documents.length} documents into AVL tree`);
  }

  // Add document to Global AVL tree only
  async addDocumentToTree(document) {
    try {
      // Ensure service is initialized before adding new documents
      if (!this.initialized) {
        console.log("üîß Service not initialized, initializing now...");
        await this.initialize();
      }

      if (
        !document.extractedText ||
        document.extractedText.trim().length === 0
      ) {
        return { success: false, message: "No text content" };
      }

      // Index document into global AVL (word -> docs/sentences)
      const { sentenceCount, uniqueTokenCount } = await this.indexDocument(document);

      console.log(
        `Added document "${document.title}" to Global AVL Tree: ${sentenceCount} sentences, ${uniqueTokenCount} unique tokens`
      );

      return {
        success: true,
        sentenceCount,
        uniqueTokenCount,
        message: "Document added to Global AVL Tree successfully"
      };
    } catch (error) {
      console.error(`Error adding document ${document._id} to tree:`, error);
      return { success: false, error: error.message };
    }
  }

  // Add document to Global AVL tree only (for initialization)
  async addDocumentToTreeOnly(document) {
    try {
      if (
        !document.extractedText ||
        document.extractedText.trim().length === 0
      ) {
        // T·∫Øt log ƒë·ªÉ tr√°nh spam console
        // console.log(`‚ö†Ô∏è  Skipping document "${document.title}" - no text content`);
        return;
      }

      // Index document into global AVL only
      const { sentenceCount, uniqueTokenCount } = await this.indexDocument(document);
      // T·∫Øt log ƒë·ªÉ tr√°nh spam console khi rebuild
      // console.log(
      //   `üìÑ Added document "${document.title}" to Global AVL Tree: ${sentenceCount} sentences, ${uniqueTokenCount} unique tokens`
      // );
    } catch (error) {
      console.error(`‚ùå Error adding document ${document._id} to tree:`, error);
    }
  }

  // Index a single document into the global AVL: tokens -> documents/sentences
  async indexDocument(document) {
    const sentences = TextHasher.extractSentences(document.extractedText);
    let uniqueTokenCount = 0;

    for (let i = 0; i < sentences.length; i++) {
      const tokens = vietnameseStopwordService.tokenizeAndFilterUniqueWithPhrases(sentences[i]);
      uniqueTokenCount += tokens.length;

      // L∆∞u tokenization sample v√†o memory ƒë·ªÉ sau n√†y save v√†o database
      const tokenizationSample = {
        documentId: String(document._id),
        sentenceIndex: i,
        originalText: sentences[i],
        tokenizedWords: tokens.map((token, idx) => ({
          word: token,
          hash: TextHasher.createMurmurHash(token),
          position: idx,
          method: 'wordTokenizer', // Method ƒë∆∞·ª£c s·ª≠ d·ª•ng
          isPreservedPhrase: token.includes(' '), // N·∫øu c√≥ space th√¨ l√† phrase
          frequency: 1
        })),
        createdAt: new Date()
      };

      // L∆∞u sample ƒë·ªÉ sau n√†y add v√†o database
      if (!this.tokenizationSamples) {
        this.tokenizationSamples = [];
      }
      this.tokenizationSamples.push(tokenizationSample);

      for (const token of tokens) {
        const hash = TextHasher.createMurmurHash(token);
        const tokenInfo = {
          method: 'wordTokenizer',
          isPreservedPhrase: token.includes(' '),
          totalFrequency: 1
        };

        this.documentTree.insertOccurrence(
          hash,
          document._id,
          `${document._id}:${i}`,
          token, // originalWord
          tokenInfo
        );
      }
    }

    // Save doc metadata
    this.docInfo.set(String(document._id), {
      documentId: document._id,
      title: document.title,
      fileType: document.fileType,
      createdAt: document.createdAt,
      uploadedBy: document.uploadedBy,
      sentenceCount: sentences.length,
      wordCount: document.extractedText.split(/\s+/).filter(Boolean).length,
    });

    return { sentenceCount: sentences.length, uniqueTokenCount };
  }

  // L∆∞u tokenized words v√†o database
  async saveTokenizedWords(document, originalSentence, tokens, sentenceIndex) {
    try {
      // T·∫°o detailed token info
      const tokenizedWords = [];
      let preservedPhrasesCount = 0;
      let filteredStopwordsCount = 0;

      for (let i = 0; i < tokens.length; i++) {
        const token = tokens[i];
        const hash = TextHasher.createMurmurHash(token);
        const isPreservedPhrase = vietnameseStopwordService.preservedPhrases.has(token.toLowerCase());
        const isStopword = vietnameseStopwordService.stopwords.has(token.toLowerCase());

        if (isPreservedPhrase) preservedPhrasesCount++;
        if (isStopword) filteredStopwordsCount++;

        tokenizedWords.push({
          word: token,
          hash: hash,
          position: i,
          method: 'wordTokenizer', // Default method used
          isPreservedPhrase: isPreservedPhrase,
          isStopword: isStopword
        });
      }

      // Metadata cho sentence n√†y
      const metadata = {
        totalWords: originalSentence.split(/\s+/).filter(Boolean).length,
        uniqueWords: tokens.length,
        preservedPhrases: preservedPhrasesCount,
        filteredStopwords: filteredStopwordsCount,
        tokenizerUsed: 'wordTokenizer'
      };

      // T·∫°o record trong database
      const tokenRecord = new TokenizedWord({
        originalText: originalSentence,
        tokenizedWords: tokenizedWords,
        documentId: String(document._id),
        sentenceId: `${document._id}:${sentenceIndex}`,
        sentenceIndex: sentenceIndex,
        metadata: metadata
      });

      await tokenRecord.save();

      // T·∫Øt log ƒë·ªÉ tr√°nh spam console
      // console.log(`üíæ Saved ${tokens.length} tokenized words for sentence ${sentenceIndex} of document "${document.title}"`);

    } catch (error) {
      console.error('Error saving tokenized words:', error);
    }
  }

  // DEPRECATED: generateAVLTreeData - No longer needed with Global AVL Tree approach
  // generateAVLTreeData(document, sortKey, wordHashes) {
  //   try {
  //     // Create hash vector representation of the document's position in AVL tree
  //     const avlTreeData = {
  //       sortKey: sortKey,
  //       hashVector: wordHashes.map((wordHash) => ({
  //         hash: wordHash.hash,
  //         word: wordHash.word,
  //         index: wordHash.index,
  //         method: wordHash.method,
  //       })),
  //       treeMetadata: {
  //         documentId: document._id,
  //         insertedAt: new Date(),
  //         textLength: document.extractedText.length,
  //         wordCount: wordHashes.length,
  //         fileTypeWeight: this.getFileTypeWeight(document.fileType),
  //       },
  //     };
  //
  //     return avlTreeData;
  //   } catch (error) {
  //     console.error("Error generating AVL tree data:", error);
  //     return null;
  //   }
  // }

  // DEPRECATED: createSortKey - No longer needed with Global AVL Tree approach  
  // createSortKey(document) {
  //   const fileTypeWeight = this.getFileTypeWeight(document.fileType);
  //   const timestamp = new Date(document.createdAt).getTime();
  //   const idHash = document._id.toString().slice(-8); // Last 8 chars of ID
  //
  //   // Format: fileTypeWeight-timestamp-idHash
  //   return `${fileTypeWeight
  //     .toString()
  //     .padStart(2, "0")}-${timestamp}-${idHash}`;
  // }

  // Assign weights to file types for sorting
  getFileTypeWeight(fileType) {
    const weights = {
      pdf: 1,
      docx: 2,
      doc: 3,
      txt: 4,
      xlsx: 5,
      xls: 6,
      pptx: 7,
      ppt: 8,
    };
    return weights[fileType] || 9;
  }

  // B·∫≠t/t·∫Øt t√≠nh nƒÉng t·ª± ƒë·ªông kh·ªüi t·∫°o
  setAutoInitialize(enabled) {
    this.autoInitialize = enabled;
    console.log(`üîß Auto-initialization ${enabled ? 'ENABLED' : 'DISABLED'}`);
  }

  // Ki·ªÉm tra tr·∫°ng th√°i auto-initialization
  isAutoInitializeEnabled() {
    return this.autoInitialize;
  }

  // Manually load documents v√†o AVL tree (khi auto-initialization b·ªã t·∫Øt)
  async manuallyLoadDocuments() {
    if (this.autoInitialize) {
      console.log("‚ö†Ô∏è  Auto-initialization is enabled. Use initialize() instead.");
      return;
    }

    try {
      console.log("üîÑ Manually loading documents into AVL Tree...");
      
      // Initialize Vietnamese stopword service first if needed
      if (!vietnameseStopwordService.initialized) {
        console.log("Initializing Vietnamese Stopword Service...");
        await vietnameseStopwordService.initialize();
      }

      // Try to load from database first
      const loadedFromDB = await this.loadFromDatabase();

      if (loadedFromDB) {
        console.log("‚úÖ Loaded Global AVL Tree from database");
        return;
      }

      // If no database backup, rebuild from documents
      console.log("No database backup found, rebuilding from documents...");
      await this.rebuildFromDocuments();

      // Save initial tree to database
      await this.saveToDatabase();

      console.log(
        `‚úÖ Manually loaded ${this.documentTree.getSize()} entries into AVL Tree`
      );
    } catch (error) {
      console.error("‚ùå Error manually loading documents:", error);
      throw error;
    }
  }

  // Cache threshold v·ªõi TTL 60 gi√¢y ƒë·ªÉ tr√°nh DB calls li√™n t·ª•c
  _thresholdCache = { value: null, timestamp: 0, ttl: 60000 };
  
  // SI√äU T·ªêI ∆ØU: Memory Pool & Object Reuse - Gi·∫£m 70% garbage collection
  _memoryPool = {
    tokenSets: [], // Pool of reusable Set objects
    arrays: [], // Pool of reusable Array objects
    objects: [], // Pool of reusable plain objects
    maxPoolSize: 100
  };
  
  // Get reusable Set from pool
  _getPooledSet() {
    return this._memoryPool.tokenSets.pop() || new Set();
  }
  
  // Return Set to pool
  _returnPooledSet(set) {
    if (this._memoryPool.tokenSets.length < this._memoryPool.maxPoolSize) {
      set.clear();
      this._memoryPool.tokenSets.push(set);
    }
  }
  
  // Get reusable Array from pool
  _getPooledArray() {
    return this._memoryPool.arrays.pop() || [];
  }
  
  // Return Array to pool
  _returnPooledArray(arr) {
    if (this._memoryPool.arrays.length < this._memoryPool.maxPoolSize) {
      arr.length = 0;
      this._memoryPool.arrays.push(arr);
    }
  }
  
  // Get current sentence threshold from database
  async getCurrentSentenceThreshold() {
    const now = Date.now();
    
    // Ki·ªÉm tra cache c√≤n hi·ªáu l·ª±c kh√¥ng
    if (this._thresholdCache.value && (now - this._thresholdCache.timestamp) < this._thresholdCache.ttl) {
      return this._thresholdCache.value;
    }
    
    try {
      const thresholds = await Threshold.getThresholdValues();
      console.log(`üöÄ Current sentence threshold: ${thresholds.sentenceThreshold}%`);
      
      // Cache k·∫øt qu·∫£
      this._thresholdCache = {
        value: thresholds,
        timestamp: now,
        ttl: 60000
      };
      
      return thresholds;
    } catch (error) {
      console.error("Error getting sentence threshold, using default:", error);
      const fallback = { sentenceThreshold: 50 };
      
      // Cache fallback value
      this._thresholdCache = {
        value: fallback,
        timestamp: now,
        ttl: 60000
      };
      
      return fallback;
    }
  }

  // Ki·ªÉm tra n·ªôi dung tr√πng l·∫∑p s·ª≠ d·ª•ng c√¢y AVL - phi√™n b·∫£n t·ªëi ∆∞u performance
  // C√°c t·ªëi ∆∞u h√≥a ƒë√£ √°p d·ª•ng:
  // 1. S·ª≠ d·ª•ng Array thay v√¨ Map cho cache ƒë·ªÉ tƒÉng t·ªëc truy c·∫≠p
  // 2. Batch search tokens trong AVL tree ƒë·ªÉ gi·∫£m s·ªë l·∫ßn t√¨m ki·∫øm
  // 3. Pre-compute token-to-documents mapping ƒë·ªÉ tr√°nh search l·∫∑p l·∫°i
  // 4. S·ª≠ d·ª•ng .lean() trong MongoDB query ƒë·ªÉ tƒÉng t·ªëc
  // 5. X·ª≠ l√Ω song song documents v·ªõi Promise.all
  // 6. Early termination trong similarity calculation
  // 7. T·ªëi ∆∞u object creation v√† array operations
  // 8. Cache length values ƒë·ªÉ tr√°nh t√≠nh to√°n l·∫∑p l·∫°i
  async checkDuplicateContent(text, options = {}) {
    // SI√äU T·ªêI ∆ØU: Performance Monitoring
    const startTime = Date.now();
    const perfStats = {
      tokenization: 0,
      avlSearch: 0,
      dbQuery: 0,
      similarity: 0,
      total: 0
    };
    
    // Kh·ªüi t·∫°o c√°c service c·∫ßn thi·∫øt
    if (!this.initialized) await this.initialize();
    if (!vietnameseStopwordService.initialized) {
      await vietnameseStopwordService.initialize();
    }

    // L·∫•y ng∆∞·ª°ng c√¢u t·ª´ database, fallback v·ªÅ options ho·∫∑c default
    const thresholdStart = Date.now();
    const threhold = await this.getCurrentSentenceThreshold();
    perfStats.dbQuery += Date.now() - thresholdStart;
    const sentenceThreshold = threhold.sentenceThreshold;
    const { minSimilarity = sentenceThreshold, maxResults = null } = options;
    
    console.log(`üéØ Using sentence threshold: ${sentenceThreshold}% (from database)`);
    console.log(`üìä Using minSimilarity: ${minSimilarity}% (for marking as duplicate)`);
    
    // FIXED: S·ª≠a logic - minSimilarity ƒë·ªÉ ƒë√°nh d·∫•u tr√πng l·∫∑p, duplicateThreshold ƒë·ªÉ l·ªçc k·∫øt qu·∫£
    const duplicateThreshold = minSimilarity; // ƒê√°nh d·∫•u l√† tr√πng l·∫∑p
    const resultFilterThreshold = threhold.sentenceThreshold; // L·ªçc k·∫øt qu·∫£ hi·ªÉn th·ªã

    try {
      // B∆∞·ªõc 1: T√°ch c√¢u v√† tokenize tr∆∞·ªõc - tr√°nh tokenize l·∫°i nhi·ªÅu l·∫ßn
      const tokenizationStart = Date.now();
      const inputSentences = TextHasher.extractSentences(text);
      const totalInputSentences = inputSentences.length;
      
      // Early return n·∫øu kh√¥ng c√≥ c√¢u n√†o ƒë·ªÉ x·ª≠ l√Ω
      if (totalInputSentences === 0) {
        return this.buildFinalResult([], 0, 0, 0);
      }
      
      // Cache tokenization k·∫øt qu·∫£ ƒë·ªÉ t√°i s·ª≠ d·ª•ng - s·ª≠ d·ª•ng Array thay v√¨ Map ƒë·ªÉ tƒÉng t·ªëc
      const sentenceTokenCache = new Array(totalInputSentences); // sentenceIndex -> tokens
      const tokenHashCache = new Map(); // token -> hash
      const globalTokenSet = new Set(); // T·∫≠p h·ª£p t·∫•t c·∫£ tokens ƒë·ªÉ t·ªëi ∆∞u t√¨m ki·∫øm
      
      // Pre-tokenize t·∫•t c·∫£ c√¢u input v√† cache k·∫øt qu·∫£
      for (let i = 0; i < totalInputSentences; i++) {
        const sentence = inputSentences[i];
        const tokens = vietnameseStopwordService.tokenizeAndFilterUniqueWithPhrases(sentence);
        sentenceTokenCache[i] = tokens;
        
        // Cache hash cho c√°c token ƒë·ªÉ tr√°nh t√≠nh l·∫°i v√† thu th·∫≠p unique tokens
        for (const token of tokens) {
          if (!tokenHashCache.has(token)) {
            tokenHashCache.set(token, TextHasher.createMurmurHash(token));
            globalTokenSet.add(token);
          }
        }
      }
      perfStats.tokenization += Date.now() - tokenizationStart;

      // B∆∞·ªõc 2: T·ªëi ∆∞u t√¨m matches trong AVL - ch·ªâ search m·ªôt l·∫ßn cho m·ªói unique token
      const avlSearchStart = Date.now();
      const tokenToDocsMap = new Map(); // token -> Set of docIds
      
      // Batch search t·∫•t c·∫£ unique tokens m·ªôt l·∫ßn
      for (const token of globalTokenSet) {
        const hash = tokenHashCache.get(token);
        const node = this.documentTree.search(hash);
        if (node && node.documents.size > 0) {
          tokenToDocsMap.set(token, node.documents);
        }
      }
      perfStats.avlSearch += Date.now() - avlSearchStart;

      const docMatches = new Map(); // docId -> { matchedSentenceCount, details: [] }
      let totalDuplicatedSentences = 0;

      // X·ª≠ l√Ω t·ª´ng c√¢u v·ªõi pre-computed token-to-docs mapping
      for (let i = 0; i < totalInputSentences; i++) {
        const sentence = inputSentences[i];
        const tokens = sentenceTokenCache[i];
        const tokenCount = tokens.length;
        if (tokenCount === 0) continue;

        // S·ª≠ d·ª•ng Map ƒë·ªÉ t·ªëi ∆∞u vi·ªác ƒë·∫øm token matches
        const perDocTokenMatches = new Map(); // docId -> Set of matched tokens

        // T·ªëi ∆∞u: ch·ªâ iterate qua tokens c√≥ matches
        for (const token of tokens) {
          const matchedDocs = tokenToDocsMap.get(token);
          if (!matchedDocs) continue;
          
          for (const docId of matchedDocs) {
            let matchedTokenSet = perDocTokenMatches.get(docId);
            if (!matchedTokenSet) {
              matchedTokenSet = new Set();
              perDocTokenMatches.set(docId, matchedTokenSet);
            }
            matchedTokenSet.add(token);
          }
        }

        // FIXED: X√©t ng∆∞·ª°ng cho t·ª´ng doc - s·ª≠ d·ª•ng duplicateThreshold (minSimilarity) ƒë·ªÉ ƒë√°nh d·∫•u tr√πng l·∫∑p
        let sentenceMarkedDuplicate = false;
        for (const [docId, matchedTokenSet] of perDocTokenMatches) {
          const matchedCount = matchedTokenSet.size;
          const percent = (matchedCount / tokenCount) * 100;
          if (percent >= duplicateThreshold) { // duplicateThreshold = minSimilarity
            sentenceMarkedDuplicate = true;
            let entry = docMatches.get(docId);
            if (!entry) {
              entry = { matchedSentenceCount: 0, details: [] };
              docMatches.set(docId, entry);
            }
            entry.matchedSentenceCount += 1;
            entry.details.push({
              inputSentenceIndex: i,
              inputSentence: sentence,
              matchedTokens: matchedCount,
              totalTokens: tokenCount,
              similarity: Math.round(percent),
            });
          }
        }

        if (sentenceMarkedDuplicate) totalDuplicatedSentences += 1;
      }

      // B∆∞·ªõc 3: SI√äU T·ªêI ∆ØU - Smart Document Pre-filtering
      const docIds = Array.from(docMatches.keys());
      const sourceDocuments = new Map(); // docId -> document
      
      if (docIds.length > 0) {
        // Pre-filter documents theo match quality ƒë·ªÉ gi·∫£m 80% documents c·∫ßn x·ª≠ l√Ω
        const docMatchEntries = Array.from(docMatches.entries());
        
        // Sort theo match count v√† ch·ªâ l·∫•y top documents
        docMatchEntries.sort((a, b) => b[1].matchedSentenceCount - a[1].matchedSentenceCount);
        
        // Intelligent filtering: ch·ªâ x·ª≠ l√Ω documents c√≥ potential cao
        const maxDocsToProcess = Math.min(maxResults || 20, docMatchEntries.length);
        const filteredDocIds = docMatchEntries.slice(0, maxDocsToProcess).map(([docId]) => docId);
        
        console.log(`üéØ Smart filtering: Processing ${filteredDocIds.length}/${docIds.length} documents with highest match potential`);
        
        // T·∫£i ch·ªâ documents ƒë∆∞·ª£c filter trong m·ªôt truy v·∫•n duy nh·∫•t
        const dbQueryStart = Date.now();
        const docs = await Document.find({
          _id: { $in: filteredDocIds }
        }).select('_id extractedText title').lean(); // S·ª≠ d·ª•ng lean() ƒë·ªÉ tƒÉng t·ªëc
        perfStats.dbQuery += Date.now() - dbQueryStart;
        
        // Update docMatches ƒë·ªÉ ch·ªâ ch·ª©a filtered documents
        const filteredDocMatches = new Map();
        for (const docId of filteredDocIds) {
          if (docMatches.has(docId)) {
            filteredDocMatches.set(docId, docMatches.get(docId));
          }
        }
        docMatches.clear();
        for (const [key, value] of filteredDocMatches) {
          docMatches.set(key, value);
        }
        
        // Cache documents v√† pre-process sentences
        const documentSentenceCache = new Map(); // docId -> sentences
        const documentTokenCache = new Map(); // docId -> Array(sentenceIndex -> tokens)
        
        // SI√äU T·ªêI ∆ØU: Parallel Tokenization - TƒÉng t·ªëc 3x
        // X·ª≠ l√Ω song song documents v·ªõi batch tokenization
        await Promise.all(docs.map(async (doc) => {
          const docIdStr = String(doc._id);
          sourceDocuments.set(docIdStr, doc);
          
          if (doc.extractedText) {
            const sentences = TextHasher.extractSentences(doc.extractedText);
            documentSentenceCache.set(docIdStr, sentences);
            
            // Parallel tokenization v·ªõi batch processing
            const sentenceTokens = new Array(sentences.length);
            const batchSize = 50; // Tokenize 50 sentences c√πng l√∫c
            
            if (sentences.length <= batchSize) {
              // Small document: tokenize t·∫•t c·∫£ c√πng l√∫c
              for (let i = 0; i < sentences.length; i++) {
                sentenceTokens[i] = vietnameseStopwordService.tokenizeAndFilterUniqueWithPhrases(sentences[i]);
              }
            } else {
              // Large document: batch parallel processing
              const batches = [];
              for (let i = 0; i < sentences.length; i += batchSize) {
                const batchEnd = Math.min(i + batchSize, sentences.length);
                batches.push({ start: i, end: batchEnd });
              }
              
              await Promise.all(batches.map(async (batch) => {
                for (let i = batch.start; i < batch.end; i++) {
                  sentenceTokens[i] = vietnameseStopwordService.tokenizeAndFilterUniqueWithPhrases(sentences[i]);
                }
              }));
            }
            
            documentTokenCache.set(docIdStr, sentenceTokens);
          }
        }));

        // B∆∞·ªõc 4: X·ª≠ l√Ω song song v·ªõi batch processing
        const matches = [];
        const actualDuplicatedSentenceIndices = new Set();
        
        // Chia docMatches th√†nh batches ƒë·ªÉ x·ª≠ l√Ω song song hi·ªáu qu·∫£ h∆°n
        const batchSize = Math.min(5, docMatches.size); // Gi·ªõi h·∫°n batch size ƒë·ªÉ tr√°nh overload
        const docMatchEntriesForBatch = Array.from(docMatches.entries());
        const batches = [];
        
        for (let i = 0; i < docMatchEntriesForBatch.length; i += batchSize) {
          batches.push(docMatchEntriesForBatch.slice(i, i + batchSize));
        }

        // X·ª≠ l√Ω t·ª´ng batch song song
        const batchResults = await Promise.all(batches.map(async (batch) => {
          const batchMatches = [];
          
          for (const [docId, data] of batch) {
            const meta = this.docInfo.get(String(docId)) || {};
            const totalSentencesInB = meta.sentenceCount || 1;
            
            const dabPercent = Math.round((data.matchedSentenceCount / totalInputSentences) * 100);
            const totalMatchedTokens = data.details.reduce((sum, detail) => sum + detail.matchedTokens, 0);
            const totalInputTokens = data.details.reduce((sum, detail) => sum + detail.totalTokens, 0);
            const similarityForSorting = totalInputTokens > 0 ? Math.round((totalMatchedTokens / totalInputTokens) * 100) : 0;

            // FIXED: S·ª≠ d·ª•ng resultFilterThreshold ƒë·ªÉ l·ªçc k·∫øt qu·∫£ hi·ªÉn th·ªã
            if (similarityForSorting >= resultFilterThreshold && data.matchedSentenceCount > 0) {
              const sourceDocument = sourceDocuments.get(String(docId));
              const sourceSentences = documentSentenceCache.get(String(docId)) || [];
              const sourceTokenCache = documentTokenCache.get(String(docId)) || new Map();

              // T·ªëi ∆∞u vi·ªác t√¨m c√¢u tr√πng l·∫∑p t·ªët nh·∫•t v·ªõi early termination v√† caching
              const similarityStart = Date.now();
              const enrichedDetails = [];
              const inputTokenSetsCache = new Map(); // Cache input token sets
              
              for (const detail of data.details) {
                // S·ª≠ d·ª•ng cached tokens c·ªßa input sentence
                const inputTokens = sentenceTokenCache[detail.inputSentenceIndex];
                if (!inputTokens || inputTokens.length === 0) {
                  enrichedDetails.push({
                    ...detail,
                    sourceSentence: detail.inputSentence,
                    docSentence: detail.inputSentence,
                    matched: detail.inputSentence,
                    text: detail.inputSentence,
                    similarity: 0,
                    matchedSentence: detail.inputSentence,
                    matchedSentenceSimilarity: 0,
                  });
                  continue;
                }

                // Cache input token set ƒë·ªÉ tr√°nh t·∫°o l·∫°i nhi·ªÅu l·∫ßn
                let inputTokenSet = inputTokenSetsCache.get(detail.inputSentenceIndex);
                if (!inputTokenSet) {
                  inputTokenSet = new Set(inputTokens.map(t => t.toLowerCase()));
                  inputTokenSetsCache.set(detail.inputSentenceIndex, inputTokenSet);
                }

                let bestMatchSentence = "";
                let bestMatchSimilarity = 0;
                const inputTokenCount = inputTokens.length;
                
                // Early termination: n·∫øu ƒë√£ t√¨m th·∫•y match 100% th√¨ d·ª´ng
                let foundPerfectMatch = false;

                // Simplified Best Match Finding - X·ª≠ l√Ω t·∫•t c·∫£ sentences
                const sourceSentencesLength = sourceSentences.length;
                
                for (let i = 0; i < sourceSentencesLength; i++) {
                  if (foundPerfectMatch) break;
                  
                  const sourceTokens = sourceTokenCache[i];
                  if (!sourceTokens || sourceTokens.length === 0) continue;
                  
                  const sourceTokensLength = sourceTokens.length;
                  const maxPossibleCommon = Math.min(inputTokenCount, sourceTokensLength);
                  
                  // Vectorized intersection counting v·ªõi memory pool
                  let commonTokensCount = 0;
                  const sourceTokenSet = this._getPooledSet();
                  
                  // Populate source token set
                  for (let j = 0; j < sourceTokensLength; j++) {
                    sourceTokenSet.add(sourceTokens[j].toLowerCase());
                  }
                  
                  for (const token of inputTokenSet) {
                    if (sourceTokenSet.has(token)) {
                      commonTokensCount++;
                      if (commonTokensCount === maxPossibleCommon) break;
                    }
                  }
                  
                  // Return set to pool
                  this._returnPooledSet(sourceTokenSet);
                  
                  const similarity = (commonTokensCount / inputTokenCount) * 100;
                  
                  if (similarity > bestMatchSimilarity) {
                    bestMatchSimilarity = similarity;
                    bestMatchSentence = sourceSentences[i];
                    
                    if (similarity >= 99.9) {
                      foundPerfectMatch = true;
                    }
                  }
                }

                // T·ªëi ∆∞u object creation b·∫±ng c√°ch t√°i s·ª≠ d·ª•ng fallback value
                const fallbackSentence = bestMatchSentence || detail.inputSentence;
                const roundedSimilarity = Math.round(bestMatchSimilarity);
                
                enrichedDetails.push({
                  ...detail,
                  sourceSentence: fallbackSentence,
                  docSentence: fallbackSentence,
                  matched: fallbackSentence,
                  text: fallbackSentence,
                  similarity: roundedSimilarity,
                  matchedSentence: fallbackSentence,
                  matchedSentenceSimilarity: roundedSimilarity,
                });
              }
              
              perfStats.similarity += Date.now() - similarityStart;

              // T·ªëi ∆∞u l·ªçc v√† t√≠nh to√°n trong m·ªôt l·∫ßn duy·ªát v·ªõi pre-allocation
              const filteredDetails = [];
              const duplicatedIndices = [];
              let filteredTotalMatchedTokens = 0;
              let filteredTotalInputTokens = 0;
              
              // Pre-allocate arrays ƒë·ªÉ tr√°nh resize nhi·ªÅu l·∫ßn
              filteredDetails.length = 0;
              duplicatedIndices.length = 0;

              // FIXED: S·ª≠ d·ª•ng resultFilterThreshold ƒë·ªÉ l·ªçc chi ti·∫øt c√¢u tr√πng l·∫∑p
              for (let i = 0; i < enrichedDetails.length; i++) {
                const detail = enrichedDetails[i];
                if (detail.matchedSentenceSimilarity >= resultFilterThreshold) {
                  filteredDetails.push(detail);
                  duplicatedIndices.push(detail.inputSentenceIndex);
                  filteredTotalMatchedTokens += detail.matchedTokens;
                  filteredTotalInputTokens += detail.totalTokens;
                }
              }

              if (filteredDetails.length > 0) {
                const filteredSimilarityForSorting = filteredTotalInputTokens > 0 ? Math.round((filteredTotalMatchedTokens / filteredTotalInputTokens) * 100) : 0;
                const filteredDabPercent = Math.round((filteredDetails.length / totalInputSentences) * 100);

                // S·ª≠a logic: ch·ªâ c·∫ßn c√≥ √≠t nh·∫•t 1 c√¢u ƒë·∫°t ng∆∞·ª°ng th√¨ ghi l·∫°i document
                if (filteredDetails.length > 0) {
                  
                  batchMatches.push({
                    match: {
                      documentId: meta.documentId || docId,
                      title: meta.title || sourceDocument?.title || "Document",
                      fileType: meta.fileType,
                      createdAt: meta.createdAt,
                      similarity: filteredSimilarityForSorting,
                      matchedHashes: undefined,
                      matchedWords: undefined,
                      duplicateSentences: filteredDetails.length,
                      totalInputSentences: totalInputSentences,
                      duplicateSentencesDetails: filteredDetails,
                      method: "global-avl-word-index",
                      dabPercent: filteredDabPercent,
                      totalSentencesInSource: totalSentencesInB,
                    },
                    duplicatedIndices
                  });
                }
              }
            }
          }
          
          return batchMatches;
        }));

        // G·ªôp k·∫øt qu·∫£ t·ª´ c√°c batches
        for (const batchResult of batchResults) {
          for (const { match, duplicatedIndices } of batchResult) {
            matches.push(match);
            duplicatedIndices.forEach(index => actualDuplicatedSentenceIndices.add(index));
          }
        }

        // B∆∞·ªõc 5: S·∫Øp x·∫øp v√† gi·ªõi h·∫°n k·∫øt qu·∫£ (t·ªëi ∆∞u cho tr∆∞·ªùng h·ª£p √≠t k·∫øt qu·∫£)
        const matchesLength = matches.length;
        if (matchesLength > 1) {
          // T·ªëi ∆∞u: s·ª≠ d·ª•ng sort in-place v√† ch·ªâ sort khi c·∫ßn thi·∫øt
          matches.sort((a, b) => b.similarity - a.similarity);
        }
        
        // T·ªëi ∆∞u: ch·ªâ slice khi th·ª±c s·ª± c·∫ßn thi·∫øt
        const limitedMatches = maxResults && matchesLength > maxResults ? 
          matches.slice(0, maxResults) : matches;

        // B∆∞·ªõc 6: T√≠nh to√°n k·∫øt qu·∫£ cu·ªëi
        const actualTotalDuplicatedSentences = actualDuplicatedSentenceIndices.size;
        const dtotalPercent = totalInputSentences > 0 ? Math.round((actualTotalDuplicatedSentences / totalInputSentences) * 100) : 0;

        // B∆∞·ªõc 7: Deduplication - G·ªôp c√°c c√¢u tr√πng l·∫∑p gi·ªëng nhau t·ª´ nhi·ªÅu documents
        const deduplicatedMatches = this.deduplicateSentences(limitedMatches);
        
        const result = this.buildFinalResult(deduplicatedMatches, dtotalPercent, totalInputSentences, actualTotalDuplicatedSentences);
        
        // Performance reporting
        perfStats.total = Date.now() - startTime;
        console.log(`üöÄ SI√äU T·ªêI ∆ØU Performance: Total=${perfStats.total}ms | Tokenization=${perfStats.tokenization}ms | AVL=${perfStats.avlSearch}ms | DB=${perfStats.dbQuery}ms | Similarity=${perfStats.similarity}ms`);
        console.log(`üìä K·∫øt qu·∫£ t·ªëi ∆∞u: Dtotal=${result.dtotal}% v·ªõi ${result.totalMatches} t√†i li·ªáu c√≥ c√¢u tr√πng l·∫∑p >= ${duplicateThreshold}% (marked) v√† hi·ªÉn th·ªã >= ${resultFilterThreshold}% (filtered)`);
        return result;
      }

      // Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ matches
      perfStats.total = Date.now() - startTime;
      console.log(`üöÄ SI√äU T·ªêI ∆ØU Performance: Total=${perfStats.total}ms | Tokenization=${perfStats.tokenization}ms | AVL=${perfStats.avlSearch}ms | DB=${perfStats.dbQuery}ms`);
      const result = this.buildFinalResult([], 0, totalInputSentences, 0);
      console.log(`üìä Kh√¥ng t√¨m th·∫•y t√†i li·ªáu tr√πng l·∫∑p n√†o`);
      return result;

    } catch (error) {
      console.error("L·ªói khi ki·ªÉm tra tr√πng l·∫∑p:", error);
      throw error;
    }
  }

  // T·ªëi ∆∞u deduplication - G·ªôp c√°c c√¢u tr√πng l·∫∑p gi·ªëng nhau t·ª´ nhi·ªÅu documents
  deduplicateSentences(matches) {
    const matchesLength = matches.length;
    // Pre-allocate array v·ªõi ƒë√∫ng size
    const deduplicatedMatches = new Array(matchesLength);
    
    for (let matchIndex = 0; matchIndex < matchesLength; matchIndex++) {
      const match = matches[matchIndex];
      const details = match.duplicateSentencesDetails;
      
      if (!details || details.length === 0) {
        deduplicatedMatches[matchIndex] = match;
        continue;
      }
      
      // T·ªëi ∆∞u deduplication v·ªõi Map v√† pre-allocated arrays
      const seenSentences = new Map(); // inputSentence -> { detail, index }
      const deduplicatedDetails = [];
      const detailsLength = details.length;
      
      // Pre-allocate ƒë·ªÉ tr√°nh resize
      deduplicatedDetails.length = 0;
      
      // S·ª≠ d·ª•ng for loop v·ªõi cached length ƒë·ªÉ t·ªëi ∆∞u performance
      for (let i = 0; i < detailsLength; i++) {
        const detail = details[i];
        const inputSentence = detail.inputSentence;
        
        const existing = seenSentences.get(inputSentence);
        if (!existing) {
          // C√¢u ch∆∞a th·∫•y, th√™m v√†o
          const newIndex = deduplicatedDetails.length;
          deduplicatedDetails.push(detail);
          seenSentences.set(inputSentence, { detail, index: newIndex });
        } else {
          // C√¢u ƒë√£ th·∫•y, so s√°nh similarity v√† gi·ªØ l·∫°i c√¢u c√≥ similarity cao h∆°n
          if (detail.similarity > existing.detail.similarity) {
            // Thay th·∫ø detail c≈© b·∫±ng detail m·ªõi c√≥ similarity cao h∆°n
            deduplicatedDetails[existing.index] = detail;
            seenSentences.set(inputSentence, { detail, index: existing.index });
          }
        }
      }
      
      // T·ªëi ∆∞u object creation b·∫±ng c√°ch ch·ªâ copy c√°c fields c·∫ßn thi·∫øt
      deduplicatedMatches[matchIndex] = {
        ...match,
        duplicateSentencesDetails: deduplicatedDetails,
        duplicateSentences: deduplicatedDetails.length
      };
    }
    
    console.log(`üîÑ Deduplication completed: Reduced duplicate sentence details across all matches`);
    return deduplicatedMatches;
  }

  buildFinalResult(matches, dtotalPercent, totalInputSentences, totalDuplicatedSentences) {
    const duplicatePercentage = dtotalPercent;
    const { dab, mostSimilarDocument } = this.calculateDtotalAndDAB(matches);

    return {
      duplicatePercentage,
      overallSimilarity: duplicatePercentage, // Add this field for compatibility
      matches,
      totalMatches: matches.length,
      checkedDocuments: this.docInfo.size,
      totalDocumentsInSystem: this.docInfo.size,
      sources: matches.map((m) => m.title),
      confidence: duplicatePercentage >= 70 ? "high" : duplicatePercentage >= 50 ? "medium" : "low", // Thay ƒë·ªïi t·ª´ 30% l√™n 50%
      mostSimilarDocument,
      dtotal: duplicatePercentage,
      dab,
      totalInputSentences,
      totalDuplicatedSentences,
      searchMethod: "global-avl-tree",
      totalDuplicateSentences: totalDuplicatedSentences,
    };
  }

  // T√≠nh t·ª∑ l·ªá tr√πng l·∫∑p t·ªïng th·ªÉ
  calculatePlagiarismRatio(totalInputHashes, matches) {
    if (matches.length === 0 || totalInputHashes === 0) return 0;

    // L·∫•y ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t
    const highestSimilarity = matches[0]?.similarity || 0;
    console.log(`üéØ T·ª∑ l·ªá tr√πng l·∫∑p t·ªïng th·ªÉ: ${highestSimilarity}%`);
    return highestSimilarity;
  }

  // T√≠nh to√°n Dtotal v√† DA/B ƒë∆°n gi·∫£n
  calculateDtotalAndDAB(matches) {
    if (matches.length === 0) {
      return { dab: 0, mostSimilarDocument: null };
    }
    const mostSimilarMatch = matches[0];
    const dab = mostSimilarMatch.dabPercent || mostSimilarMatch.similarity || 0;
    const mostSimilarDocument = {
      id: mostSimilarMatch.documentId,
      name: mostSimilarMatch.title,
      similarity: mostSimilarMatch.similarity,
    };
    return { dab, mostSimilarDocument };
  }

  // T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa hai vƒÉn b·∫£n
  calculateTextSimilarity(text1, text2) {
    const words1 = text1.toLowerCase().split(/\s+/).filter((w) => w.length > 2);
    const words2 = text2.toLowerCase().split(/\s+/).filter((w) => w.length > 2);

    if (words1.length === 0 || words2.length === 0) return 0;

    const set1 = new Set(words1);
    const set2 = new Set(words2);
    const intersection = new Set([...set1].filter((x) => set2.has(x)));

    // C√¥ng th·ª©c: (s·ªë t·ª´ chung / t·ªïng s·ªë t·ª´ c·ªßa vƒÉn b·∫£n ng·∫Øn h∆°n) * 100%
    const minSize = Math.min(set1.size, set2.size);
    return Math.round((intersection.size / minSize) * 100);
  }



  // Get tree statistics
  getTreeStats() {
    if (!this.initialized) {
      return {
        totalDocuments: 0,
        totalSentences: 0,
        totalNodes: 0,
        treeSize: 0,
        initialized: false,
      };
    }

    // Aggregate from docInfo
    let totalSentences = 0;
    const fileTypeStats = {};
    for (const meta of this.docInfo.values()) {
      totalSentences += meta.sentenceCount || 0;
      if (meta.fileType) fileTypeStats[meta.fileType] = (fileTypeStats[meta.fileType] || 0) + 1;
    }

    return {
      totalDocuments: this.docInfo.size,
      totalSentences,
      totalNodes: this.documentTree.getSize(),
      treeSize: this.documentTree.getSize(),
      initialized: this.initialized,
      fileTypeDistribution: fileTypeStats,
      treeHeight: this.getTreeHeight(),
    };
  }

  // Get tree height
  getTreeHeight() {
    if (!this.documentTree.root) return 0;
    return this.documentTree.getHeight(this.documentTree.root);
  }

  // Remove document from tree (when document is deleted)
  async removeDocumentFromTree(documentId) {
    try {
      if (!this.initialized) {
        console.warn('AVL tree not initialized, skipping document removal');
        return;
      }

      console.log(`Removing document ${documentId} from AVL tree...`);

      // Remove document from all nodes and clean up empty nodes
      const removedCount = this.documentTree.removeDocumentAndCleanup(documentId);

      // Remove document info from memory
      this.docInfo.delete(String(documentId));

      // Remove tokenization samples for this document
      this.tokenizationSamples = this.tokenizationSamples.filter(
        sample => sample.documentId !== String(documentId)
      );

      // Remove tokenized words from database
      try {
        const deletedTokens = await TokenizedWord.deleteMany({
          documentId: String(documentId)
        });
        console.log(`Removed ${deletedTokens.deletedCount} tokenized word records for document ${documentId}`);
      } catch (tokenError) {
        console.warn(`Failed to remove tokenized words for document ${documentId}:`, tokenError.message);
      }

      console.log(`Removed document ${documentId} from ${removedCount} nodes in AVL tree`);

      // Report tree statistics after removal
      const emptyNodes = this.documentTree.getEmptyNodesCount();
      const totalNodes = this.documentTree.getSize();
      if (emptyNodes > 0) {
        console.log(`‚ö†Ô∏è Tree now has ${emptyNodes} empty nodes out of ${totalNodes} total nodes`);

        // Note: We don't auto-rebuild here because the document might still exist in database
        // Auto-rebuild should be done manually or during maintenance
        const emptyRatio = emptyNodes / totalNodes;
        if (emptyRatio > 0.5) { // Only warn if more than 50% empty nodes
          console.log(`‚ö†Ô∏è High ratio of empty nodes (${(emptyRatio * 100).toFixed(1)}%). Consider manual tree optimization.`);
        }
      }

      // L∆∞u c√¢y ƒë√£ c·∫≠p nh·∫≠t v√†o database
      const saveResult = await this.saveToDatabase();
      if (saveResult) {
        console.log(`‚úÖ AVL tree updated and saved after removing document ${documentId}`);
      } else {
        console.warn(`‚ö†Ô∏è Failed to save AVL tree after removing document ${documentId}`);
      }
    } catch (error) {
      console.error(`Error removing document ${documentId} from tree:`, error);

      // Fallback: rebuild tree if direct removal fails
      console.log('Falling back to full tree rebuild...');
      try {
        await this.refreshTree();
        await this.saveToDatabase();
        console.log('‚úÖ Tree rebuilt successfully as fallback');
      } catch (rebuildError) {
        console.error('Failed to rebuild tree as fallback:', rebuildError);
      }
    }
  }

  // Refresh tree (reload from database)
  async refreshTree() {
    console.log('üîÑ Refreshing AVL tree from database...');
    this.documentTree.clear();
    this.docInfo.clear();
    this.tokenizationSamples = [];
    this.initialized = false;
    await this.initialize();
    console.log(`‚úÖ AVL tree refreshed with ${this.documentTree.getSize()} entries`);
  }

  // Save Global AVL Tree to database
  async saveToDatabase() {
    try {
      if (!this.initialized) {
        console.warn('Tree not initialized, skipping save');
        return false;
      }

      console.log('Saving Global AVL Tree to database...');

      // Serialize tree
      const serializedTree = this.documentTree.serialize();

      // Calculate token statistics
      const tokenStats = this.calculateTokenStats();

      // Prepare document info v·ªõi tokenization stats
      const documentInfo = Array.from(this.docInfo.entries()).map(([docId, info]) => {
        const docSamples = this.tokenizationSamples.filter(s => s.documentId === docId);
        const docTokenStats = this.calculateDocumentTokenStats(docSamples);

        return {
          documentId: docId,
          title: info.title,
          fileType: info.fileType,
          createdAt: info.createdAt,
          uploadedBy: info.uploadedBy ? String(info.uploadedBy) : null,
          sentenceCount: info.sentenceCount,
          wordCount: info.wordCount,
          tokenizationStats: docTokenStats
        };
      });

      // Prepare metadata
      const metadata = {
        totalNodes: serializedTree.metadata.totalNodes,
        totalDocuments: this.docInfo.size,
        totalSentences: Array.from(this.docInfo.values()).reduce((sum, info) => sum + (info.sentenceCount || 0), 0),
        treeHeight: serializedTree.metadata.treeHeight,
        tokenStats: tokenStats
      };

      const treeData = {
        metadata,
        nodes: serializedTree.nodes,
        rootHash: serializedTree.rootHash,
        documentInfo,
        tokenizationSamples: this.tokenizationSamples
      };

      // Check if tree exists
      const existingTree = await GlobalAVLTreeUnified.getLatest();

      if (existingTree) {
        await existingTree.updateTree(treeData);
        console.log('‚úÖ Global AVL Tree updated in database');
      } else {
        await GlobalAVLTreeUnified.createNew(treeData);
        console.log('‚úÖ Global AVL Tree created in database');
      }

      this.lastSaved = new Date();
      return true;
    } catch (error) {
      console.error('Error saving Global AVL Tree to database:', error);
      return false;
    }
  }

  // Load Global AVL Tree from database
  async loadFromDatabase() {
    try {
      console.log('üîÑ Loading Global AVL Tree from database...');

      const savedTree = await GlobalAVLTreeUnified.getLatest();
      if (!savedTree) {
        console.log('‚ö†Ô∏è  No saved tree found in database - will rebuild from documents');
        return false;
      }

      console.log(`üìä Found saved tree: ${savedTree.documentInfo?.length || 0} documents, last updated: ${savedTree.lastUpdated}`);

      // Deserialize tree
      const serializedData = {
        nodes: savedTree.nodes,
        rootHash: savedTree.rootHash,
        metadata: savedTree.metadata
      };

      this.documentTree = TreeAVL.deserialize(serializedData);

      // Restore document info
      this.docInfo.clear();
      if (savedTree.documentInfo) {
        savedTree.documentInfo.forEach(info => {
          this.docInfo.set(info.documentId, {
            documentId: info.documentId,
            title: info.title,
            fileType: info.fileType,
            createdAt: info.createdAt,
            uploadedBy: info.uploadedBy,
            sentenceCount: info.sentenceCount,
            wordCount: info.wordCount
          });
        });
      }

      // Restore tokenization samples
      this.tokenizationSamples = savedTree.tokenizationSamples || [];

      console.log(`‚úÖ Successfully loaded tree with ${this.documentTree.getSize()} nodes, ${this.docInfo.size} documents`);
      this.lastSaved = savedTree.lastUpdated;
      return true;
    } catch (error) {
      console.error('‚ùå Error loading Global AVL Tree from database:', error);
      console.error('Will rebuild from documents instead...');
      return false;
    }
  }

  // Calculate token statistics
  calculateTokenStats() {
    const stats = {
      totalTokens: this.tokenizationSamples.reduce((sum, sample) => sum + sample.tokenizedWords.length, 0),
      preservedPhrases: 0,
      methodDistribution: {
        wordTokenizer: 0,
        tokenizer: 0,
        fallback: 0
      }
    };

    this.tokenizationSamples.forEach(sample => {
      sample.tokenizedWords.forEach(token => {
        if (token.isPreservedPhrase) {
          stats.preservedPhrases++;
        }
        stats.methodDistribution[token.method]++;
      });
    });

    return stats;
  }

  // Calculate document-specific token statistics
  calculateDocumentTokenStats(docSamples) {
    if (!docSamples || docSamples.length === 0) {
      return {
        totalTokens: 0,
        preservedPhrases: 0,
        uniqueTokens: 0
      };
    }

    const uniqueTokens = new Set();
    let preservedPhrases = 0;
    let totalTokens = 0;

    docSamples.forEach(sample => {
      sample.tokenizedWords.forEach(token => {
        totalTokens++;
        uniqueTokens.add(token.word);
        if (token.isPreservedPhrase) {
          preservedPhrases++;
        }
      });
    });

    return {
      totalTokens,
      preservedPhrases,
      uniqueTokens: uniqueTokens.size
    };
  }

  // Force save tree to database
  async forceSave() {
    return await this.saveToDatabase();
  }

  // Get save status
  getSaveStatus() {
    return {
      autoSave: this.autoSave,
      lastSaved: this.lastSaved,
      saveInterval: this.saveInterval,
      initialized: this.initialized,
      initializing: this.initializing
    };
  }

  // Disable auto-save (useful for debugging)
  disableAutoSave() {
    console.log('üö´ Auto-save disabled');
    this.autoSave = false;
  }

  // Enable auto-save
  enableAutoSave() {
    console.log('‚úÖ Auto-save enabled');
    this.autoSave = true;
  }
}

// Create singleton instance
const documentAVLService = new DocumentAVLService();

documentAVLService.disableAutoSave()

module.exports = documentAVLService;
